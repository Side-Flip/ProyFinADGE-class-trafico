{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77bb7373-aa3b-4c1e-9ad0-bae2b86b81db",
   "metadata": {},
   "source": [
    "# Importar jars y definir spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d446090d-a2c4-46e5-9179-7ed0991751b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/25 03:47:36 WARN SparkConf: The configuration key 'spark.yarn.executor.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.executor.memoryOverhead' instead.\n",
      "25/11/25 03:47:36 WARN SparkConf: The configuration key 'spark.yarn.executor.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.executor.memoryOverhead' instead.\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/11/25 03:47:36 WARN SparkConf: The configuration key 'spark.yarn.executor.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.executor.memoryOverhead' instead.\n",
      "25/11/25 03:47:36 WARN SparkConf: The configuration key 'spark.yarn.executor.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.executor.memoryOverhead' instead.\n",
      "25/11/25 03:47:36 WARN SparkConf: The configuration key 'spark.yarn.executor.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.executor.memoryOverhead' instead.\n",
      "25/11/25 03:47:37 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/11/25 03:47:39 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/11/25 03:47:42 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "# --- 1. Configuraci√≥n ---\n",
    "os.environ['SPARK_HOME'] = \"/home/hadoop/spark\"\n",
    "sys.path.insert(0, \"/home/hadoop/spark/python\")\n",
    "sys.path.insert(0, \"/home/hadoop/spark/python/lib/py4j-0.10.9.7-src.zip\") \n",
    "sys.path.insert(0, \"/home/hadoop/spark/python/lib/pyspark.zip\")\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Ingesta_Limpia_Multiclase\") \\\n",
    "    .master(\"yarn\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.cores\", \"2\") \\\n",
    "    .config(\"spark.yarn.executor.memoryOverhead\", \"1024m\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"‚úÖ Spark iniciado correctamente usando los JARs del sistema.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87adf5de-881b-4e2c-89be-cf1b7884a75e",
   "metadata": {},
   "source": [
    "# Procesar datos en formato csv y guardar en nueva carpeta\n",
    "## Se procesan 9 archivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bebc429c-fe6d-4067-bde0-18359b6dc5ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /trafico_clean\n",
      "Procesando 9 archivos...\n",
      "\n",
      "--> Procesando: Friday-02-03-2018_TrafficForML_CICFlowMeter.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    üìâ Columnas eliminadas: 13 (De 80 iniciales)\n",
      "    ‚úÇÔ∏è  Registros eliminados: 0 (Headers repetidos o basura)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/21 21:05:07 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /trafico/Friday-02-03-2018_TrafficForML_CICFlowMeter.csv\n",
      "    ‚úÖ Guardado y origen eliminado.\n",
      "\n",
      "--> Procesando: Friday-16-02-2018_TrafficForML_CICFlowMeter.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    üìâ Columnas eliminadas: 13 (De 80 iniciales)\n",
      "    ‚úÇÔ∏è  Registros eliminados: 1 (Headers repetidos o basura)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /trafico/Friday-16-02-2018_TrafficForML_CICFlowMeter.csv\n",
      "    ‚úÖ Guardado y origen eliminado.\n",
      "\n",
      "--> Procesando: Friday-23-02-2018_TrafficForML_CICFlowMeter.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    üìâ Columnas eliminadas: 13 (De 80 iniciales)\n",
      "    ‚úÇÔ∏è  Registros eliminados: 0 (Headers repetidos o basura)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /trafico/Friday-23-02-2018_TrafficForML_CICFlowMeter.csv\n",
      "    ‚úÖ Guardado y origen eliminado.\n",
      "\n",
      "--> Procesando: Thursday-01-03-2018_TrafficForML_CICFlowMeter.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    üìâ Columnas eliminadas: 13 (De 80 iniciales)\n",
      "    ‚úÇÔ∏è  Registros eliminados: 25 (Headers repetidos o basura)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /trafico/Thursday-01-03-2018_TrafficForML_CICFlowMeter.csv\n",
      "    ‚úÖ Guardado y origen eliminado.\n",
      "\n",
      "--> Procesando: Thursday-15-02-2018_TrafficForML_CICFlowMeter.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    üìâ Columnas eliminadas: 13 (De 80 iniciales)\n",
      "    ‚úÇÔ∏è  Registros eliminados: 0 (Headers repetidos o basura)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /trafico/Thursday-15-02-2018_TrafficForML_CICFlowMeter.csv\n",
      "    ‚úÖ Guardado y origen eliminado.\n",
      "\n",
      "--> Procesando: Thursday-22-02-2018_TrafficForML_CICFlowMeter.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    üìâ Columnas eliminadas: 13 (De 80 iniciales)\n",
      "    ‚úÇÔ∏è  Registros eliminados: 0 (Headers repetidos o basura)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /trafico/Thursday-22-02-2018_TrafficForML_CICFlowMeter.csv\n",
      "    ‚úÖ Guardado y origen eliminado.\n",
      "\n",
      "--> Procesando: Wednesday-14-02-2018_TrafficForML_CICFlowMeter.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    üìâ Columnas eliminadas: 13 (De 80 iniciales)\n",
      "    ‚úÇÔ∏è  Registros eliminados: 0 (Headers repetidos o basura)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /trafico/Wednesday-14-02-2018_TrafficForML_CICFlowMeter.csv\n",
      "    ‚úÖ Guardado y origen eliminado.\n",
      "\n",
      "--> Procesando: Wednesday-21-02-2018_TrafficForML_CICFlowMeter.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    üìâ Columnas eliminadas: 13 (De 80 iniciales)\n",
      "    ‚úÇÔ∏è  Registros eliminados: 0 (Headers repetidos o basura)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /trafico/Wednesday-21-02-2018_TrafficForML_CICFlowMeter.csv\n",
      "    ‚úÖ Guardado y origen eliminado.\n",
      "\n",
      "--> Procesando: Wednesday-28-02-2018_TrafficForML_CICFlowMeter.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    üìâ Columnas eliminadas: 13 (De 80 iniciales)\n",
      "    ‚úÇÔ∏è  Registros eliminados: 33 (Headers repetidos o basura)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /trafico/Wednesday-28-02-2018_TrafficForML_CICFlowMeter.csv\n",
      "    ‚úÖ Guardado y origen eliminado.\n",
      "\n",
      "‚úÖ Proceso completado.\n"
     ]
    }
   ],
   "source": [
    "# Rutas\n",
    "CARPETA_ORIGEN = \"/trafico\"\n",
    "CARPETA_DESTINO = \"/trafico_clean\"\n",
    "\n",
    "\n",
    "# Columnas a eliminar\n",
    "cols_drop_base = [\n",
    "    \"Bwd PSH Flags\", \"Bwd URG Flags\", \"Fwd Pkts/b Avg\", \"Bwd Pkts/b Avg\",\n",
    "    \"Fwd Byts/b Avg\", \"Bwd Byts/b Avg\", \"Fwd Blk Rate Avg\", \"Bwd Blk Rate Avg\",\n",
    "    \"Fwd URG Flags\", \"CWE Flag Count\", \"FIN Flag Cnt\", \"Timestamp\",\n",
    "    \"Flow ID\", \"Src IP\", \"Dst IP\", \"Src Port\", \"Protocol\"\n",
    "]\n",
    "\n",
    "# Obtener lista de archivos\n",
    "try:\n",
    "    cmd = f\"hdfs dfs -ls {CARPETA_ORIGEN} | grep .csv | awk '{{print $8}}'\"\n",
    "    lista_archivos = subprocess.check_output(cmd, shell=True).decode(\"utf-8\").strip().split(\"\\n\")\n",
    "    lista_archivos = [x for x in lista_archivos if x]\n",
    "except:\n",
    "    lista_archivos = []\n",
    "\n",
    "print(f\"Procesando {len(lista_archivos)} archivos...\")\n",
    "\n",
    "for archivo in lista_archivos:\n",
    "    filename = archivo.split(\"/\")[-1]\n",
    "    print(f\"\\n--> Procesando: {filename}\")\n",
    "    \n",
    "    try:\n",
    "        # 1. Leer CSV\n",
    "        df_temp = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"false\").csv(archivo)\n",
    "        \n",
    "        # --- METRICAS INICIALES ---\n",
    "        # Contamos filas iniciales (esto toma unos segundos extra)\n",
    "        filas_iniciales = df_temp.count()\n",
    "        \n",
    "        # 2. Limpiar filas (Eliminar headers repetidos 'Label')\n",
    "        df_temp = df_temp.filter(col(\"Label\") != \"Label\")\n",
    "        \n",
    "        # --- METRICAS DE FILAS ---\n",
    "        filas_finales = df_temp.count()\n",
    "        filas_borradas = filas_iniciales - filas_finales\n",
    "        \n",
    "        # 3. Limpiar Columnas\n",
    "        cols_existentes = df_temp.columns\n",
    "        # Identificamos la intersecci√≥n (cu√°les de la lista est√°n realmente en el archivo)\n",
    "        cols_a_borrar = [c for c in cols_drop_base if c in cols_existentes]\n",
    "        df_temp = df_temp.drop(*cols_a_borrar)\n",
    "        \n",
    "        # --- IMPRIMIR REPORTE ---\n",
    "        print(f\"    üìâ Columnas eliminadas: {len(cols_a_borrar)} (De {len(cols_existentes)} iniciales)\")\n",
    "        print(f\"    ‚úÇÔ∏è  Registros eliminados: {filas_borradas} (Headers repetidos o basura)\")\n",
    "        \n",
    "        # 4. Castear a Double (MANTENIENDO ETIQUETA STRING)\n",
    "        for c in df_temp.columns:\n",
    "            if c != \"Label\":\n",
    "                df_temp = df_temp.withColumn(c, col(c).cast(DoubleType()))\n",
    "        \n",
    "        # (La l√≠nea binaria est√° comentada/omitida como pediste)\n",
    "        # df_temp = df_temp.withColumn(\"Label_Binary\", ...)\n",
    "        \n",
    "        # 5. Guardar\n",
    "        df_temp.write.mode(\"append\").parquet(CARPETA_DESTINO)\n",
    "        \n",
    "        # 6. Borrar original\n",
    "        subprocess.check_call(f\"hdfs dfs -rm -skipTrash {archivo}\", shell=True)\n",
    "        print(f\"    ‚úÖ Guardado y origen eliminado.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"    ‚ùå Error: {str(e)}\")\n",
    "        spark.stop()\n",
    "        break\n",
    "\n",
    "print(\"\\n‚úÖ Proceso completado.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19aafe21-4128-49dc-8090-80cbf6994471",
   "metadata": {},
   "source": [
    "# Se procesa el archivo de 3.7 GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd32399-8564-4c77-bfb7-a001aa1cff3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rutas\n",
    "ARCHIVO_PESADO = \"/trafico/Thuesday-20-02-2018_TrafficForML_CICFlowMeter.csv\"\n",
    "CARPETA_DESTINO = \"/trafico_clean\"\n",
    "\n",
    "# Columnas a eliminar (Misma lista que los otros)\n",
    "cols_drop_base = [\n",
    "    \"Bwd PSH Flags\", \"Bwd URG Flags\", \"Fwd Pkts/b Avg\", \"Bwd Pkts/b Avg\",\n",
    "    \"Fwd Byts/b Avg\", \"Bwd Byts/b Avg\", \"Fwd Blk Rate Avg\", \"Bwd Blk Rate Avg\",\n",
    "    \"Fwd URG Flags\", \"CWE Flag Count\", \"FIN Flag Cnt\", \"Timestamp\",\n",
    "    \"Flow ID\", \"Src IP\", \"Dst IP\", \"Src Port\", \"Protocol\"\n",
    "]\n",
    "\n",
    "print(f\"üöÄ Procesando archivo pesado para recuperar datos...\")\n",
    "\n",
    "try:\n",
    "    # 1. Leer\n",
    "    # Nota: Sin inferSchema es vital para este archivo de 3.8GB\n",
    "    df_temp = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"false\").csv(ARCHIVO_PESADO)\n",
    "    \n",
    "    print(f\"    Le√≠do. Filas: {df_temp.count():,}\")\n",
    "\n",
    "    # 2. Limpiar\n",
    "    df_temp = df_temp.filter(col(\"Label\") != \"Label\")\n",
    "    \n",
    "    cols_existentes = df_temp.columns\n",
    "    cols_a_borrar = [c for c in cols_drop_base if c in cols_existentes]\n",
    "    df_temp = df_temp.drop(*cols_a_borrar)\n",
    "\n",
    "    # 3. Castear\n",
    "    for c in df_temp.columns:\n",
    "        if c != \"Label\":\n",
    "            df_temp = df_temp.withColumn(c, col(c).cast(DoubleType()))\n",
    "    \n",
    "    # 4. APPEND (Crucial: modo append para sumar a los otros archivos)\n",
    "    df_temp.write.mode(\"append\").parquet(CARPETA_DESTINO)\n",
    "    \n",
    "    # 5. Borrar CSV origen\n",
    "    subprocess.check_call(f\"hdfs dfs -rm -skipTrash {ARCHIVO_PESADO}\", shell=True)\n",
    "    \n",
    "    print(f\"‚úÖ RECUPERACI√ìN COMPLETADA. El archivo pesado se uni√≥ al dataset.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {str(e)}\")\n",
    "\n",
    "# Verificaci√≥n final\n",
    "total = spark.read.parquet(CARPETA_DESTINO).count()\n",
    "print(f\"üìä Total registros en el dataset completo: {total:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42c0478-da28-460d-aa9c-9394157855fc",
   "metadata": {},
   "source": [
    "# Verificar datos procesados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fef2470d-5044-4060-8835-c397cbf31f6b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Inspeccionando dataset en: /trafico_clean\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìè Dimensiones: 16,232,943 filas x 67 columnas\n",
      "‚úÖ Estructura Correcta: Todas las features son num√©ricas.\n",
      "\n",
      "üè∑Ô∏è Generando √çndices de Clase (Por frecuencia)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/21 22:36:07 WARN SparkConf: The configuration key 'spark.yarn.executor.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.executor.memoryOverhead' instead.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä DISTRIBUCI√ìN DE CLASES (Mapeo e Histograma):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------------+--------+\n",
      "|Label_Index|Label                   |count   |\n",
      "+-----------+------------------------+--------+\n",
      "|0.0        |Benign                  |13484708|\n",
      "|1.0        |DDOS attack-HOIC        |686012  |\n",
      "|2.0        |DDoS attacks-LOIC-HTTP  |576191  |\n",
      "|3.0        |DoS attacks-Hulk        |461912  |\n",
      "|4.0        |Bot                     |286191  |\n",
      "|5.0        |FTP-BruteForce          |193360  |\n",
      "|6.0        |SSH-Bruteforce          |187589  |\n",
      "|7.0        |Infilteration           |161934  |\n",
      "|8.0        |DoS attacks-SlowHTTPTest|139890  |\n",
      "|9.0        |DoS attacks-GoldenEye   |41508   |\n",
      "|10.0       |DoS attacks-Slowloris   |10990   |\n",
      "|11.0       |DDOS attack-LOIC-UDP    |1730    |\n",
      "|12.0       |Brute Force -Web        |611     |\n",
      "|13.0       |Brute Force -XSS        |230     |\n",
      "|14.0       |SQL Injection           |87      |\n",
      "+-----------+------------------------+--------+\n",
      "\n",
      "\n",
      "se Buscando valores Nulos o NaN en todas las columnas...\n",
      "(Esto puede tardar un poco dependiendo del tama√±o...)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/21 22:36:47 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå SE ENCONTRARON ERRORES:\n",
      "   - Flow Byts/s: 59721 nulos\n",
      "\n",
      "üïµÔ∏è  Buscando columnas constantes (in√∫tiles para IA)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 15:>                                                         (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Todas las columnas parecen tener variaci√≥n de datos.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, count, when, isnan, countDistinct\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "PATH_DATA = \"/trafico_clean\"\n",
    "print(f\"üîç Inspeccionando dataset en: {PATH_DATA}\")\n",
    "\n",
    "#definir df y cargar datos\n",
    "df = spark.read.parquet(PATH_DATA)\n",
    "\n",
    "#dimenciones\n",
    "total_filas = df.count()\n",
    "total_cols = len(df.columns)\n",
    "print(f\"\\nüìè Dimensiones: {total_filas:,} filas x {total_cols} columnas\")\n",
    "\n",
    "#verificar tipo de dato Double y label se descarta\n",
    "tipos = dict(df.dtypes)\n",
    "no_numericas = [c for c, t in tipos.items() if t == 'string' and c != 'Label']\n",
    "if no_numericas:\n",
    "    print(f\"‚ö†Ô∏è ALERTA: Hay columnas string que no deber√≠an estar: {no_numericas}\")\n",
    "else:\n",
    "    print(\"‚úÖ Estructura Correcta: Todas las features son num√©ricas.\")\n",
    "\n",
    "#definir nueva columna xon tipo int\n",
    "print(\"\\nüè∑Ô∏è Generando √çndices de Clase (Por frecuencia)...\")\n",
    "#asignar indices de forma ascendente de mayor a menor cantidad de clase \n",
    "indexer = StringIndexer(inputCol=\"Label\", outputCol=\"Label_Index\")\n",
    "indexer_model = indexer.fit(df)\n",
    "df_indexed = indexer_model.transform(df)\n",
    "\n",
    "print(\"\\nüìä DISTRIBUCI√ìN DE CLASES (Mapeo e Histograma):\")\n",
    "#tabla de cols e indices\n",
    "df_stats = df_indexed.groupBy(\"Label_Index\", \"Label\").count().orderBy(\"Label_Index\")\n",
    "df_stats.show(30, truncate=False)\n",
    "\n",
    "#Verificar nulos \n",
    "print(\"\\nse Buscando valores Nulos o NaN en todas las columnas...\")\n",
    "print(\"(Esto puede tardar un poco dependiendo del tama√±o...)\")\n",
    "\n",
    "expresiones_nulos = []\n",
    "for c in df.columns:\n",
    "    if c == \"Label\": continue\n",
    "    #es null o nan\n",
    "    expresiones_nulos.append(count(when(col(c).isNull() | isnan(col(c)), c)).alias(c))\n",
    "\n",
    "df_nulos = df.select(expresiones_nulos)\n",
    "\n",
    "#verificar errores \n",
    "nulos_dict = df_nulos.first().asDict()\n",
    "columnas_con_nulos = {k: v for k, v in nulos_dict.items() if v > 0}\n",
    "\n",
    "if not columnas_con_nulos:\n",
    "    print(\"‚úÖ EXCELENTE: No se encontraron valores nulos ni NaN en el dataset.\")\n",
    "else:\n",
    "    print(\"‚ùå SE ENCONTRARON ERRORES:\")\n",
    "    for col_name, cant in columnas_con_nulos.items():\n",
    "        print(f\"   - {col_name}: {cant} nulos\")\n",
    "\n",
    "#mayormente cero\n",
    "print(\"\\nüïµÔ∏è  Buscando columnas constantes (in√∫tiles para IA)...\")\n",
    "# desvest\n",
    "from pyspark.sql.functions import stddev\n",
    "\n",
    "features_check = [c for c in df.columns if c not in [\"Label\", \"Label_Index\"]]\n",
    "#analizar el 10\n",
    "df_sample = df.sample(fraction=0.1, seed=42)\n",
    "resumen_stats = df_sample.select([stddev(c).alias(c) for c in features_check]).first().asDict()\n",
    "\n",
    "constantes = [c for c, val in resumen_stats.items() if val == 0 or val is None]\n",
    "\n",
    "if constantes:\n",
    "    print(f\"‚ö†Ô∏è  ADVERTENCIA: Las siguientes {len(constantes)} columnas tienen varianza 0 (son constantes):\")\n",
    "    print(constantes)\n",
    "    print(\"   -> Sugerencia: Elim√≠nalas antes de hacer la matriz de correlaci√≥n.\")\n",
    "else:\n",
    "    print(\"‚úÖ Todas las columnas parecen tener variaci√≥n de datos.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c1c7123-03c0-4b21-b0c3-b9b0d543866c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El dataset est√°: 19 particiones\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.parquet(\"/trafico_clean\")\n",
    "num_particiones = df.rdd.getNumPartitions()\n",
    "print(f\"El dataset est√°: {num_particiones} particiones\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c121384-9fbb-4cf8-ba57-3b565294efe9",
   "metadata": {},
   "source": [
    "# Eliminar registors nulos encontrados "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1331d74-414b-4639-adf9-bf57c9691100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Leyendo datos desde: /trafico_clean\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Filtrando registros corruptos en 'Flow Byts/s'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Se detectaron 59721 registros sucios (NaN/Null).\n",
      "üíæ Guardando dataset corregido en carpeta temporal...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/21 23:02:03 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Reemplazando carpeta original...\n",
      "Deleted /trafico_clean\n",
      "üéâ ¬°Listo! Tu dataset en /trafico_clean ahora tiene 16,173,222 filas limpias.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, isnan\n",
    "\n",
    "# Rutas\n",
    "RUTA_ORIGINAL = \"/trafico_clean\"\n",
    "RUTA_TEMP = \"/trafico_clean_temp\"\n",
    "\n",
    "print(f\"üîÑ Leyendo datos desde: {RUTA_ORIGINAL}\")\n",
    "df = spark.read.parquet(RUTA_ORIGINAL)\n",
    "total_inicial = df.count()\n",
    "\n",
    "#eliminar nans\n",
    "col_problema = \"Flow Byts/s\"\n",
    "\n",
    "print(f\"üîç Filtrando registros corruptos en '{col_problema}'...\")\n",
    "\n",
    "# no borrar lo que interesa\n",
    "df_limpio = df.filter(\n",
    "    col(col_problema).isNotNull() & (~isnan(col(col_problema)))\n",
    ")\n",
    "\n",
    "total_final = df_limpio.count()\n",
    "borrados = total_inicial - total_final\n",
    "\n",
    "if borrados == 0:\n",
    "    print(\"‚ö†Ô∏è No se encontraron registros para borrar. Verifica el nombre de la columna.\")\n",
    "else:\n",
    "    print(f\"‚úÖ Se detectaron {borrados} registros sucios (NaN/Null).\")\n",
    "    \n",
    "    #guardar en temporarl \n",
    "    print(\"üíæ Guardando dataset corregido en carpeta temporal...\")\n",
    "    df_limpio.write.mode(\"overwrite\").parquet(RUTA_TEMP)\n",
    "    \n",
    "    #reemplazar\n",
    "    print(\"üîÑ Reemplazando carpeta original...\")\n",
    "    # borrar viejos nulos \n",
    "    subprocess.check_call(f\"hdfs dfs -rm -r -skipTrash {RUTA_ORIGINAL}\", shell=True)\n",
    "    #renombrar\n",
    "    subprocess.check_call(f\"hdfs dfs -mv {RUTA_TEMP} {RUTA_ORIGINAL}\", shell=True)\n",
    "    \n",
    "    print(f\"üéâ ¬°Listo! Tu dataset en {RUTA_ORIGINAL} ahora tiene {total_final:,} filas limpias.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0a055e0-46b6-4462-934a-7b1a334991ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/21 23:13:00 WARN SparkConf: The configuration key 'spark.yarn.executor.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.executor.memoryOverhead' instead.\n",
      "25/11/21 23:13:00 WARN SparkConf: The configuration key 'spark.yarn.executor.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.executor.memoryOverhead' instead.\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/11/21 23:13:00 WARN SparkConf: The configuration key 'spark.yarn.executor.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.executor.memoryOverhead' instead.\n",
      "25/11/21 23:13:00 WARN SparkConf: The configuration key 'spark.yarn.executor.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.executor.memoryOverhead' instead.\n",
      "25/11/21 23:13:00 WARN SparkConf: The configuration key 'spark.yarn.executor.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.executor.memoryOverhead' instead.\n",
      "25/11/21 23:13:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/11/21 23:13:05 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Verificando dataset en: /trafico_clean\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ TOTAL DE REGISTROS LIMPIOS: 16,173,222\n",
      "Verificando columnas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/21 23:14:04 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "[Stage 4:======================================================>  (18 + 1) / 19]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "No hay nulos ni NaN en ninguna columna.\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count, when, isnan\n",
    "\n",
    "# --- 1. Configuraci√≥n ---\n",
    "os.environ['SPARK_HOME'] = \"/home/hadoop/spark\"\n",
    "sys.path.insert(0, \"/home/hadoop/spark/python\")\n",
    "sys.path.insert(0, \"/home/hadoop/spark/python/lib/py4j-0.10.9.7-src.zip\") \n",
    "sys.path.insert(0, \"/home/hadoop/spark/python/lib/pyspark.zip\")\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Verificacion_Final_Nulos\") \\\n",
    "    .master(\"yarn\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.cores\", \"2\") \\\n",
    "    .config(\"spark.yarn.executor.memoryOverhead\", \"1024m\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "RUTA_DATOS = \"/trafico_clean\"\n",
    "print(f\"üîç Verificando dataset en: {RUTA_DATOS}\")\n",
    "\n",
    "# Cargar datos\n",
    "df = spark.read.parquet(RUTA_DATOS)\n",
    "\n",
    "# ============================================================\n",
    "# 1. CONTEO TOTAL DE REGISTROS\n",
    "# ============================================================\n",
    "total = df.count()\n",
    "print(f\"\\n‚úÖ TOTAL DE REGISTROS LIMPIOS: {total:,}\")\n",
    "\n",
    "# ============================================================\n",
    "# 2. B√öSQUEDA INTENSIVA DE NULOS\n",
    "# ============================================================\n",
    "print(\"Verificando columnas\")\n",
    "\n",
    "exprs = []\n",
    "for c in df.columns:\n",
    "    # La columna Label es String, solo verificamos isNull\n",
    "    if c == \"Label\":\n",
    "        condicion = col(c).isNull()\n",
    "    # Las dem√°s son Double, verificamos isNull O isNaN\n",
    "    else:\n",
    "        condicion = col(c).isNull() | isnan(col(c))\n",
    "    \n",
    "    exprs.append(count(when(condicion, c)).alias(c))\n",
    "\n",
    "# Ejecutamos la b√∫squeda\n",
    "resultados = df.select(exprs).first().asDict()\n",
    "\n",
    "# Filtramos solo las que tengan errores > 0\n",
    "errores = {k: v for k, v in resultados.items() if v > 0}\n",
    "\n",
    "print(\"-\" * 50)\n",
    "if not errores:\n",
    "    print(\"No hay nulos ni NaN en ninguna columna.\")\n",
    "else:\n",
    "    print(\"A√∫n quedan valores sucios:\")\n",
    "    for col_name, qty in errores.items():\n",
    "        print(f\"   - {col_name}: {qty} registros malos\")\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c64ef5-d66d-49ed-8c0b-0c2492dd051f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet(\"/trafico_clean\")\n",
    "df_sample_cache = df.sample(fraction=0.01, seed=42).limit(100000).cache()\n",
    "print(\"‚úÖ Datos cargados y muestra en cach√©.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306b047b-3bd3-4453-999b-3dbfa4a4d370",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "print(\"üîç Iniciando detecci√≥n de columnas duplicadas...\")\n",
    "\n",
    "# 1. OBTENER MUESTRA (Fase de Sospecha)\n",
    "# Bajamos 50k filas a Pandas. Es r√°pido y suficiente para detectar patrones.\n",
    "print(\"    ‚¨áÔ∏è  Bajando muestra para an√°lisis r√°pido...\")\n",
    "df_sample = df.sample(fraction=0.1, seed=42).limit(50000).toPandas()\n",
    "\n",
    "# 2. IDENTIFICAR CANDIDATOS (L√≥gica en Python/Pandas)\n",
    "# Transponemos la muestra (ahora las columnas son filas) y buscamos duplicados\n",
    "print(\"    üïµÔ∏è  Buscando columnas id√©nticas en la muestra...\")\n",
    "df_T = df_sample.T\n",
    "duplicated_mask = df_T.duplicated()\n",
    "columnas_duplicadas_names = df_T[duplicated_mask].index.tolist()\n",
    "\n",
    "# Ahora necesitamos saber \"qui√©n es copia de qui√©n\"\n",
    "# Creamos un mapa: {Columna_Borrable: Columna_Original}\n",
    "mapa_duplicados = {}\n",
    "# Iteramos sobre las columnas marcadas como duplicadas\n",
    "for col_dup in columnas_duplicadas_names:\n",
    "    # Buscamos la primera columna que tenga los mismos valores que esta\n",
    "    # (Pandas .duplicated() marca True a partir de la segunda aparici√≥n)\n",
    "    # Esto es un poco fuerza bruta en local, pero con 50k filas es instant√°neo.\n",
    "    valores_dup = df_sample[col_dup]\n",
    "    for col_orig in df_sample.columns:\n",
    "        if col_orig == col_dup: break # No compararse consigo misma\n",
    "        if df_sample[col_orig].equals(valores_dup):\n",
    "            mapa_duplicados[col_dup] = col_orig\n",
    "            break\n",
    "\n",
    "print(f\"    ‚ö†Ô∏è  Candidatos encontrados en la muestra: {len(mapa_duplicados)}\")\n",
    "for copia, original in mapa_duplicados.items():\n",
    "    print(f\"       Posible copia: '{copia}'  <-- Id√©ntica a --> '{original}'\")\n",
    "\n",
    "# 3. VERIFICACI√ìN BLINDADA (En Spark completo)\n",
    "# Ahora verificamos si son id√©nticas en los 16 MILLONES de filas, no solo en la muestra.\n",
    "print(\"\\nüî¨ Verificando exactitud en todo el Big Data (Spark)...\")\n",
    "\n",
    "columnas_a_eliminar_final = []\n",
    "\n",
    "for copia, original in mapa_duplicados.items():\n",
    "    # La l√≥gica: Contamos cu√°ntas filas son DIFERENTES. \n",
    "    # Si el conteo es 0, son gemelas perfectas.\n",
    "    # Manejamos nulos con 'eqNullSafe' (igualdad segura ante nulos)\n",
    "    diferencias = df.filter(col(copia).eqNullSafe(col(original)) == False).count()\n",
    "    \n",
    "    if diferencias == 0:\n",
    "        print(f\"    ‚úÖ CONFIRMADO: '{copia}' es duplicada exacta de '{original}'. Se eliminar√°.\")\n",
    "        columnas_a_eliminar_final.append(copia)\n",
    "    else:\n",
    "        print(f\"    ‚ùå FALSO POSITIVO: '{copia}' y '{original}' difieren en {diferencias} filas. SE CONSERVA.\")\n",
    "\n",
    "# 4. ELIMINACI√ìN\n",
    "if columnas_a_eliminar_final:\n",
    "    print(f\"\\nüóëÔ∏è Eliminando {len(columnas_a_eliminar_final)} columnas redundantes...\")\n",
    "    df_optimizado = df.drop(*columnas_a_eliminar_final)\n",
    "    \n",
    "    print(f\"    Columnas antes: {len(df.columns)}\")\n",
    "    print(f\"    Columnas ahora: {len(df_optimizado.columns)}\")\n",
    "    \n",
    "    # Opcional: Sobreescribir variable df si quieres seguir us√°ndola\n",
    "    # df = df_optimizado\n",
    "    \n",
    "    # Guardar (Recomendado)\n",
    "    # df_optimizado.write.mode(\"overwrite\").parquet(\"/trafico_sin_duplicados\")\n",
    "    # print(\"    üíæ Dataset optimizado guardado.\")\n",
    "else:\n",
    "    print(\"\\n‚ú® ¬°Felicidades! No tienes columnas duplicadas exactas.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
