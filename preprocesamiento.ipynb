{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bebc429c-fe6d-4067-bde0-18359b6dc5ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/20 17:05:53 WARN SparkConf: The configuration key 'spark.yarn.executor.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.executor.memoryOverhead' instead.\n",
      "25/11/20 17:05:53 WARN SparkConf: The configuration key 'spark.yarn.executor.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.executor.memoryOverhead' instead.\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/11/20 17:05:54 WARN SparkConf: The configuration key 'spark.yarn.executor.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.executor.memoryOverhead' instead.\n",
      "25/11/20 17:05:54 WARN SparkConf: The configuration key 'spark.yarn.executor.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.executor.memoryOverhead' instead.\n",
      "25/11/20 17:05:54 WARN SparkConf: The configuration key 'spark.yarn.executor.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.executor.memoryOverhead' instead.\n",
      "25/11/20 17:05:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/11/20 17:05:58 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Spark iniciado correctamente usando los JARs del sistema.\n",
      "--> Procesando archivo √∫nico: Thuesday-20-02-2018_TrafficForML_CICFlowMeter.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Guardando en Parquet (esto puede tardar unos minutos)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/20 17:07:03 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "25/11/20 17:07:18 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 2 for reason Container marked as failed: container_1763647897811_0017_01_000003 on host: ED3. Exit status: -100. Diagnostics: Container released on a *lost* node.\n",
      "25/11/20 17:07:18 ERROR YarnScheduler: Lost executor 2 on ED3: Container marked as failed: container_1763647897811_0017_01_000003 on host: ED3. Exit status: -100. Diagnostics: Container released on a *lost* node.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ‚úÖ Guardado exitoso.\n",
      "Deleted /trafico/Thuesday-20-02-2018_TrafficForML_CICFlowMeter.csv\n",
      "    üóëÔ∏è Archivo CSV eliminado para liberar espacio.\n",
      "\n",
      "--- Proceso finalizado ---\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "#Ubicaci√≥n de Spark OJO no pyspark\n",
    "os.environ['SPARK_HOME'] = \"/home/hadoop/spark\"\n",
    "\n",
    "#jars nativos\n",
    "sys.path.insert(0, \"/home/hadoop/spark/python\")\n",
    "sys.path.insert(0, \"/home/hadoop/spark/python/lib/py4j-0.10.9.7-src.zip\") \n",
    "sys.path.insert(0, \"/home/hadoop/spark/python/lib/pyspark.zip\")\n",
    "\n",
    "#Importar Spark con jars\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "#Usar yarn\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Limpieza_Manual_Jars\") \\\n",
    "    .master(\"yarn\") \\\n",
    "    .config(\"spark.executor.memory\", \"1g\") \\\n",
    "    .config(\"spark.executor.cores\", \"2\") \\\n",
    "    .config(\"spark.yarn.executor.memoryOverhead\", \"512m\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"‚úÖ Spark iniciado correctamente usando los JARs del sistema.\")\n",
    "\n",
    "#Definir rutas\n",
    "CARPETA_ORIGEN = \"/trafico\"\n",
    "CARPETA_DESTINO = \"/trafico_clean\"\n",
    "\n",
    "#columnas de archivo de 3.7gb\n",
    "cols_drop_base = [\n",
    "    \"Flow ID\", \"Src IP\", \"Dst IP\", \"Src Port\", \"Timestamp\", \n",
    "    \"Bwd PSH Flags\", \"Bwd URG Flags\", \"Fwd URG Flags\",\n",
    "    \"CWE Flag Count\", \"Fwd Byts/b Avg\", \"Bwd Byts/b Avg\",\n",
    "    \"Fwd Pkts/b Avg\", \"Bwd Pkts/b Avg\", \"Fwd Blk Rate Avg\", \"Bwd Blk Rate Avg\"\n",
    "]\n",
    "\n",
    "#ubicar archivo\n",
    "cmd = f\"hdfs dfs -ls {CARPETA_ORIGEN} | grep .csv | awk '{{print $8}}'\"\n",
    "try:\n",
    "    archivos_b = subprocess.check_output(cmd, shell=True)\n",
    "    lista_archivos = archivos_b.decode(\"utf-8\").strip().split(\"\\n\")\n",
    "    # Filtramos l√≠neas vac√≠as\n",
    "    lista_archivos = [x for x in lista_archivos if x]\n",
    "except:\n",
    "    lista_archivos = []\n",
    "\n",
    "if not lista_archivos:\n",
    "    print(\"‚ö†Ô∏è No se encontraron archivos .csv en /trafico\")\n",
    "else:\n",
    "    archivo_actual = lista_archivos[0] # Tomamos el √∫nico archivo\n",
    "    filename = archivo_actual.split(\"/\")[-1]\n",
    "    \n",
    "    print(f\"--> Procesando archivo √∫nico: {filename}\")\n",
    "    \n",
    "    try:\n",
    "        #memoria para que no colapse \n",
    "        df_temp = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"false\").csv(archivo_actual)\n",
    "        \n",
    "        #limpiar\n",
    "        df_temp = df_temp.filter(col(\"Label\") != \"Label\")\n",
    "        \n",
    "        cols_existentes = df_temp.columns\n",
    "        cols_a_borrar = [c for c in cols_drop_base if c in cols_existentes]\n",
    "        df_temp = df_temp.drop(*cols_a_borrar)\n",
    "        \n",
    "        #pasar cols a num√©ricas\n",
    "        numeric_cols = [c for c in df_temp.columns if c != \"Label\"]\n",
    "        for c in numeric_cols:\n",
    "            df_temp = df_temp.withColumn(c, col(c).cast(DoubleType()))\n",
    "            \n",
    "        #clases binarias\n",
    "        df_temp = df_temp.withColumn(\"Label_Binary\", when(col(\"Label\") == \"Benign\", 0.0).otherwise(1.0))\n",
    "        \n",
    "        #guardar en formato parquet\n",
    "        print(\"    Guardando en Parquet (esto puede tardar unos minutos)...\")\n",
    "        df_temp.write.mode(\"append\").parquet(CARPETA_DESTINO)\n",
    "        print(\"    ‚úÖ Guardado exitoso.\")\n",
    "        \n",
    "        #borrar csv original\n",
    "        cmd_rm = f\"hdfs dfs -rm -skipTrash {archivo_actual}\"\n",
    "        subprocess.check_call(cmd_rm, shell=True)\n",
    "        print(f\"    üóëÔ∏è Archivo CSV eliminado para liberar espacio.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"    ‚ùå ERROR: {str(e)}\")\n",
    "        spark.stop()\n",
    "\n",
    "print(\"\\n--- Proceso finalizado ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dbd91ece-282e-49cc-beba-d42d41ac1922",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leyendo dataset completo desde Parquet...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 1. Muestra del Esquema (Primeras 5 columnas + Labels) ---\n",
      "root\n",
      " |-- Dst Port: double (nullable = true)\n",
      " |-- Protocol: double (nullable = true)\n",
      " |-- Flow Duration: double (nullable = true)\n",
      " |-- Tot Fwd Pkts: double (nullable = true)\n",
      " |-- Tot Bwd Pkts: double (nullable = true)\n",
      " |-- Label: string (nullable = true)\n",
      " |-- Label_Binary: double (nullable = true)\n",
      "\n",
      "\n",
      "--- 2. Verificaci√≥n de Etiquetas ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+------------+-------+\n",
      "|Label                 |Label_Binary|count  |\n",
      "+----------------------+------------+-------+\n",
      "|Benign                |0.0         |7372557|\n",
      "|DDoS attacks-LOIC-HTTP|1.0         |576191 |\n",
      "+----------------------+------------+-------+\n",
      "\n",
      "\n",
      "--- 3. Buscando valores Nulos en columnas num√©ricas ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+-------------+------------+------------+---------------+---------------+---------------+---------------+----------------+\n",
      "|Dst Port|Protocol|Flow Duration|Tot Fwd Pkts|Tot Bwd Pkts|TotLen Fwd Pkts|TotLen Bwd Pkts|Fwd Pkt Len Max|Fwd Pkt Len Min|Fwd Pkt Len Mean|\n",
      "+--------+--------+-------------+------------+------------+---------------+---------------+---------------+---------------+----------------+\n",
      "|       0|       0|            0|           0|           0|              0|              0|              0|              0|               0|\n",
      "+--------+--------+-------------+------------+------------+---------------+---------------+---------------+---------------+----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:==============================================>           (8 + 2) / 10]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de registros en el dataset final: 7,948,748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# --- VERIFICACI√ìN FINAL DEL DATASET COMPLETO ---\n",
    "print(\"Leyendo dataset completo desde Parquet...\")\n",
    "df_final = spark.read.parquet(\"/trafico_clean\")\n",
    "\n",
    "# 1. Verificar Tipos de Dato\n",
    "print(\"\\n--- 1. Muestra del Esquema (Primeras 5 columnas + Labels) ---\")\n",
    "# Imprimimos solo una parte para no llenar la pantalla\n",
    "df_final.select(df_final.columns[:5] + [\"Label\", \"Label_Binary\"]).printSchema()\n",
    "\n",
    "# 2. Verificar Transformaci√≥n de Labels\n",
    "print(\"\\n--- 2. Verificaci√≥n de Etiquetas ---\")\n",
    "df_final.groupBy(\"Label\", \"Label_Binary\").count().show(truncate=False)\n",
    "# Deber√≠as ver:\n",
    "# Benign   | 0.0 | XXXXX\n",
    "# FTP-B... | 1.0 | XXXXX\n",
    "# SSH-B... | 1.0 | XXXXX\n",
    "# etc...\n",
    "\n",
    "# 3. Chequeo R√°pido de Nulos\n",
    "print(\"\\n--- 3. Buscando valores Nulos en columnas num√©ricas ---\")\n",
    "# Hacemos un conteo de nulos por columna. \n",
    "# (Esto puede tardar un minuto porque revisa todo el dataset)\n",
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "\n",
    "# Seleccionamos algunas columnas clave para probar (o todas si tienes paciencia)\n",
    "columnas_a_probar = df_final.columns[:10] # Probamos las primeras 10 columnas\n",
    "\n",
    "df_final.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in columnas_a_probar]).show()\n",
    "\n",
    "print(f\"Total de registros en el dataset final: {df_final.count():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d249a77-1fbd-49c8-a358-a909190a7448",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "#rutas ahora resto de archivos\n",
    "os.environ['SPARK_HOME'] = \"/home/hadoop/spark\"\n",
    "\n",
    "# rutas\n",
    "sys.path.insert(0, \"/home/hadoop/spark/python\")\n",
    "sys.path.insert(0, \"/home/hadoop/spark/python/lib/py4j-0.10.9.7-src.zip\") \n",
    "sys.path.insert(0, \"/home/hadoop/spark/python/lib/pyspark.zip\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ade1d84-5414-4e68-9871-5f36c9a3e1d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/20 18:20:36 WARN SparkConf: The configuration key 'spark.yarn.executor.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.executor.memoryOverhead' instead.\n",
      "25/11/20 18:20:36 WARN SparkConf: The configuration key 'spark.yarn.executor.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.executor.memoryOverhead' instead.\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/11/20 18:20:37 WARN SparkConf: The configuration key 'spark.yarn.executor.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.executor.memoryOverhead' instead.\n",
      "25/11/20 18:20:37 WARN SparkConf: The configuration key 'spark.yarn.executor.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.executor.memoryOverhead' instead.\n",
      "25/11/20 18:20:37 WARN SparkConf: The configuration key 'spark.yarn.executor.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.executor.memoryOverhead' instead.\n",
      "25/11/20 18:20:37 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/11/20 18:20:41 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buscando archivos en /trafico...\n",
      "Se encontraron 9 archivos.\n",
      "\n",
      "--> Procesando: Friday-02-03-2018_TrafficForML_CICFlowMeter.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    üìä Columnas iniciales: 80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/20 18:21:38 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /trafico/Friday-02-03-2018_TrafficForML_CICFlowMeter.csv\n",
      "    ‚úÖ Procesado y eliminado.\n",
      "\n",
      "--> Procesando: Friday-16-02-2018_TrafficForML_CICFlowMeter.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    üìä Columnas iniciales: 80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /trafico/Friday-16-02-2018_TrafficForML_CICFlowMeter.csv\n",
      "    ‚úÖ Procesado y eliminado.\n",
      "\n",
      "--> Procesando: Friday-23-02-2018_TrafficForML_CICFlowMeter.csv\n",
      "    üìä Columnas iniciales: 80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/20 18:23:29 ERROR TransportClient: Failed to send RPC RPC 6486607694425462776 to /10.6.101.125:40190: io.netty.channel.StacklessClosedChannelException\n",
      "io.netty.channel.StacklessClosedChannelException\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.write(Object, ChannelPromise)(Unknown Source)\n",
      "25/11/20 18:23:29 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Attempted to get executor loss reason for executor id 1 at RPC address 10.6.101.125:52224, but got no response. Marking as agent lost.\n",
      "java.io.IOException: Failed to send RPC RPC 6486607694425462776 to /10.6.101.125:40190: io.netty.channel.StacklessClosedChannelException\n",
      "\tat org.apache.spark.network.client.TransportClient$RpcChannelListener.handleFailure(TransportClient.java:395)\n",
      "\tat org.apache.spark.network.client.TransportClient$StdChannelListener.operationComplete(TransportClient.java:372)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:557)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.access$200(DefaultPromise.java:35)\n",
      "\tat io.netty.util.concurrent.DefaultPromise$1.run(DefaultPromise.java:503)\n",
      "\tat io.netty.util.concurrent.AbstractEventExecutor.runTask(AbstractEventExecutor.java:174)\n",
      "\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:167)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:470)\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:569)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)\n",
      "\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n",
      "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: io.netty.channel.StacklessClosedChannelException\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.write(Object, ChannelPromise)(Unknown Source)\n",
      "25/11/20 18:23:29 ERROR YarnScheduler: Lost executor 1 on ED5: Executor Process Lost\n",
      "25/11/20 18:23:29 WARN TaskSetManager: Lost task 3.0 in stage 5.0 (TID 14) (ED5 executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Executor Process Lost\n",
      "25/11/20 18:23:29 WARN TaskSetManager: Lost task 1.0 in stage 5.0 (TID 12) (ED5 executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Executor Process Lost\n",
      "25/11/20 18:23:29 ERROR TransportClient: Failed to send RPC RPC 5688300848901677879 to /10.6.101.125:40190: io.netty.channel.StacklessClosedChannelException\n",
      "io.netty.channel.StacklessClosedChannelException\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.write(Object, ChannelPromise)(Unknown Source)\n",
      "25/11/20 18:23:29 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Attempted to get executor loss reason for executor id 2 at RPC address 10.6.101.124:46268, but got no response. Marking as agent lost.\n",
      "java.io.IOException: Failed to send RPC RPC 5688300848901677879 to /10.6.101.125:40190: io.netty.channel.StacklessClosedChannelException\n",
      "\tat org.apache.spark.network.client.TransportClient$RpcChannelListener.handleFailure(TransportClient.java:395)\n",
      "\tat org.apache.spark.network.client.TransportClient$StdChannelListener.operationComplete(TransportClient.java:372)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:557)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:629)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:118)\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetFailure(AbstractChannel.java:999)\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.write(AbstractChannel.java:860)\n",
      "\tat io.netty.channel.DefaultChannelPipeline$HeadContext.write(DefaultChannelPipeline.java:1367)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeWrite0(AbstractChannelHandlerContext.java:877)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeWriteAndFlush(AbstractChannelHandlerContext.java:940)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext$WriteTask.run(AbstractChannelHandlerContext.java:1247)\n",
      "\tat io.netty.util.concurrent.AbstractEventExecutor.runTask(AbstractEventExecutor.java:174)\n",
      "\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:167)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:470)\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:569)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)\n",
      "\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n",
      "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: io.netty.channel.StacklessClosedChannelException\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.write(Object, ChannelPromise)(Unknown Source)\n",
      "25/11/20 18:23:29 ERROR YarnScheduler: Lost executor 2 on ED4: Executor Process Lost\n",
      "25/11/20 18:23:29 WARN TaskSetManager: Lost task 0.0 in stage 5.0 (TID 11) (ED4 executor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Executor Process Lost\n",
      "25/11/20 18:23:29 WARN TaskSetManager: Lost task 2.0 in stage 5.0 (TID 13) (ED4 executor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Executor Process Lost\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /trafico/Friday-23-02-2018_TrafficForML_CICFlowMeter.csv\n",
      "    ‚úÖ Procesado y eliminado.\n",
      "\n",
      "--> Procesando: Thursday-01-03-2018_TrafficForML_CICFlowMeter.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    üìä Columnas iniciales: 80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /trafico/Thursday-01-03-2018_TrafficForML_CICFlowMeter.csv\n",
      "    ‚úÖ Procesado y eliminado.\n",
      "\n",
      "--> Procesando: Thursday-15-02-2018_TrafficForML_CICFlowMeter.csv\n",
      "    üìä Columnas iniciales: 80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /trafico/Thursday-15-02-2018_TrafficForML_CICFlowMeter.csv\n",
      "    ‚úÖ Procesado y eliminado.\n",
      "\n",
      "--> Procesando: Thursday-22-02-2018_TrafficForML_CICFlowMeter.csv\n",
      "    üìä Columnas iniciales: 80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /trafico/Thursday-22-02-2018_TrafficForML_CICFlowMeter.csv\n",
      "    ‚úÖ Procesado y eliminado.\n",
      "\n",
      "--> Procesando: Wednesday-14-02-2018_TrafficForML_CICFlowMeter.csv\n",
      "    üìä Columnas iniciales: 80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /trafico/Wednesday-14-02-2018_TrafficForML_CICFlowMeter.csv\n",
      "    ‚úÖ Procesado y eliminado.\n",
      "\n",
      "--> Procesando: Wednesday-21-02-2018_TrafficForML_CICFlowMeter.csv\n",
      "    üìä Columnas iniciales: 80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /trafico/Wednesday-21-02-2018_TrafficForML_CICFlowMeter.csv\n",
      "    ‚úÖ Procesado y eliminado.\n",
      "\n",
      "--> Procesando: Wednesday-28-02-2018_TrafficForML_CICFlowMeter.csv\n",
      "    üìä Columnas iniciales: 80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /trafico/Wednesday-28-02-2018_TrafficForML_CICFlowMeter.csv\n",
      "    ‚úÖ Procesado y eliminado.\n",
      "\n",
      "--- Procesamiento Terminado ---\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, count, isnan\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Procesamiento_Masivo_Seguro\") \\\n",
    "    .master(\"yarn\") \\\n",
    "    .config(\"spark.executor.memory\", \"1g\") \\\n",
    "    .config(\"spark.executor.cores\", \"2\") \\\n",
    "    .config(\"spark.yarn.executor.memoryOverhead\", \"512m\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Rutas\n",
    "CARPETA_ORIGEN = \"/trafico\"\n",
    "CARPETA_DESTINO = \"/trafico_clean\"\n",
    "\n",
    "#cols eliminar \n",
    "cols_drop_base = [\n",
    "    \"Bwd PSH Flags\", \"Bwd URG Flags\", \"Fwd Pkts/b Avg\", \"Bwd Pkts/b Avg\",\n",
    "    \"Fwd Byts/b Avg\", \"Bwd Byts/b Avg\", \"Fwd Blk Rate Avg\", \"Bwd Blk Rate Avg\",\n",
    "    \"Fwd URG Flags\", \"CWE Flag Count\", \"FIN Flag Cnt\", \"Timestamp\",\n",
    "    \"Flow ID\", \"Src IP\", \"Dst IP\", \"Src Port\", \"Protocol\"\n",
    "]\n",
    "\n",
    "#buscar archivos\n",
    "print(\"Buscando archivos en /trafico...\")\n",
    "try:\n",
    "    cmd = f\"hdfs dfs -ls {CARPETA_ORIGEN} | grep .csv | awk '{{print $8}}'\"\n",
    "    lista_archivos = subprocess.check_output(cmd, shell=True).decode(\"utf-8\").strip().split(\"\\n\")\n",
    "    lista_archivos = [x for x in lista_archivos if x]\n",
    "except:\n",
    "    lista_archivos = []\n",
    "\n",
    "if not lista_archivos:\n",
    "    print(\"‚ö†Ô∏è No se encontraron archivos nuevos.\")\n",
    "else:\n",
    "    print(f\"Se encontraron {len(lista_archivos)} archivos.\")\n",
    "\n",
    "    for archivo in lista_archivos:\n",
    "        filename = archivo.split(\"/\")[-1]\n",
    "        print(f\"\\n--> Procesando: {filename}\")\n",
    "        \n",
    "        try:\n",
    "            #leer\n",
    "            df_temp = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"false\").csv(archivo)\n",
    "            \n",
    "            #buscar columnas\n",
    "            cant_cols = len(df_temp.columns)\n",
    "            print(f\"    üìä Columnas iniciales: {cant_cols}\")\n",
    "            \n",
    "            #limpiar\n",
    "            df_temp = df_temp.filter(col(\"Label\") != \"Label\")\n",
    "            cols_existentes = df_temp.columns\n",
    "            cols_a_borrar = [c for c in cols_drop_base if c in cols_existentes]\n",
    "            df_temp = df_temp.drop(*cols_a_borrar)\n",
    "            \n",
    "            #conteo\n",
    "            for c in df_temp.columns:\n",
    "                if c != \"Label\":\n",
    "                    df_temp = df_temp.withColumn(c, col(c).cast(DoubleType()))\n",
    "            \n",
    "            #label binario\n",
    "            df_temp = df_temp.withColumn(\"Label_Binary\", when(col(\"Label\") == \"Benign\", 0.0).otherwise(1.0))\n",
    "            \n",
    "            #guardar y borrar\n",
    "            df_temp.write.mode(\"append\").parquet(CARPETA_DESTINO)\n",
    "            subprocess.check_call(f\"hdfs dfs -rm -skipTrash {archivo}\", shell=True)\n",
    "            print(f\"    ‚úÖ Procesado y eliminado.\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"    ‚ùå Error: {str(e)}\")\n",
    "\n",
    "print(\"\\n--- Procesamiento Terminado ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe3bd6f3-2c84-4cb5-b544-d27861c22dca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ TOTAL DE REGISTROS RECUPERADOS: 16,232,943\n",
      "\n",
      "üìä Distribuci√≥n de Etiquetas (Debe haber Benignos y Ataques):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+\n",
      "|Label_Binary|   count|\n",
      "+------------+--------+\n",
      "|         0.0|13484708|\n",
      "|         1.0| 2748235|\n",
      "+------------+--------+\n",
      "\n",
      "\n",
      "üëÄ Primeras 5 filas:\n",
      "+------+------------+--------+-------------+\n",
      "| Label|Label_Binary|Dst Port|Flow Duration|\n",
      "+------+------------+--------+-------------+\n",
      "|Benign|         0.0|    80.0|  8.1828496E7|\n",
      "|Benign|         0.0|    80.0|  8.1758552E7|\n",
      "|Benign|         0.0|    80.0|  8.1688858E7|\n",
      "|Benign|         0.0|    80.0|  8.1618526E7|\n",
      "|Benign|         0.0|    80.0|  8.1548276E7|\n",
      "+------+------------+--------+-------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "üìè Cantidad de columnas: 68\n"
     ]
    }
   ],
   "source": [
    "# Cargar el dataset unido\n",
    "df_final = spark.read.parquet(\"/trafico_clean\")\n",
    "\n",
    "#registros totales\n",
    "total_filas = df_final.count()\n",
    "print(f\"‚úÖ TOTAL DE REGISTROS RECUPERADOS: {total_filas:,}\")\n",
    "\n",
    "#verificar clases\n",
    "print(\"\\nüìä Distribuci√≥n de Etiquetas (Debe haber Benignos y Ataques):\")\n",
    "df_final.groupBy(\"Label_Binary\").count().show()\n",
    "\n",
    "#Verificar contenido \n",
    "print(\"\\nüëÄ Primeras 5 filas:\")\n",
    "df_final.select(\"Label\", \"Label_Binary\", df_final.columns[0], df_final.columns[1]).show(5)\n",
    "\n",
    "#conteo final columnas\n",
    "print(f\"\\nüìè Cantidad de columnas: {len(df_final.columns)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
